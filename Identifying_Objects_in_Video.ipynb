{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Identifying-Objects-in-Video",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "copyright"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "foJLc1i5BoBP"
      },
      "source": [
        "#Object Recognition in Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nTirVS4FWaPx"
      },
      "source": [
        "In this project we will import a pre-existing model that recognizes objects and use the model to identify those objects in a video. We'll edit the video to draw boxes around the identified object and then reassemble the video so that the boxes are shown around objects in the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_yACB56w2JVk"
      },
      "source": [
        "## Team\n",
        "\n",
        "*   Alex de Magalhaes\n",
        "*   Lynn He\n",
        "*   Wayne Chim\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFBmqMrXCrBc",
        "colab_type": "text"
      },
      "source": [
        "##Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jhTEOK1ZmqN8"
      },
      "source": [
        "Our goal was to process a video frame-by-frame, identify objects in each frame, and draw a bounding box and label around each object.\n",
        " \n",
        "We used the pre-built model [SSD MobileNet V1 Coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) 'ssd_mobilenet_v1_coco' model. To start, we processed this video [found on Pixabay](https://pixabay.com/videos/cars-motorway-speed-motion-traffic-1900/). \n",
        " \n",
        "\n",
        "The [Coco labels file](https://github.com/nightrome/cocostuff/blob/master/labels.txt) can be used to identify classified objects.\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bct_P0IJVAWC",
        "colab_type": "text"
      },
      "source": [
        "##Skills\n",
        "* Classification\n",
        "* Saving and Loading Models\n",
        "* OpenCV\n",
        "* Video Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jJZZ2bURAAy",
        "colab_type": "text"
      },
      "source": [
        "##Importing Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRsBgCC2YAGU",
        "colab_type": "text"
      },
      "source": [
        "Importing all of our libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me9XTYosSB7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR8Th9u_WPsc",
        "colab_type": "text"
      },
      "source": [
        "This is our experimental code block where we tried working with one frame before moving on to more frames throughout the entire video."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwjXlqOMRVmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Put the name of the video you want to process here!\n",
        "video_name = 'cars.mp4'\n",
        "\n",
        "#Experimenting with different videos and individual frames\n",
        "video = cv.VideoCapture(video_name)\n",
        "video.set(cv.CAP_PROP_POS_FRAMES, 10) #123, 200\n",
        "ret, image = video.read()\n",
        "if not ret:\n",
        "  raise Exception(f'Problem reading frame {current_frame} from video')\n",
        "\n",
        "video.release()\n",
        "image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fJKkCW-YB6A",
        "colab_type": "text"
      },
      "source": [
        "Processing the entire video would be impractical, so a for loop is ran to grab the first frame in every second. Since the video lasts 60 seconds, we will be working with approximately 60 frames. All the frames will be padded to have square dimensions and then appended to a list that will be inputted into the model later on. We also added the frames to a list to pass into the model. By running only one TensorFlow session, we save time through parellelization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54G3KoP9q1lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "video = cv.VideoCapture(video_name)\n",
        "\n",
        "fps = int(video.get(cv.CAP_PROP_FPS))\n",
        "total_frame = int(video.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "input_images = []\n",
        "#In this case, one frame per second is retrieved from the video as part of the testing list\n",
        "for i in range(0, total_frame, fps):\n",
        "  video.set(cv.CAP_PROP_POS_FRAMES, i)\n",
        "  ret, image = video.read()\n",
        "  image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "  height = image.shape[0]\n",
        "  width = image.shape[1]\n",
        "  \n",
        "  #Padding is applied to each frame for more friendly formats\n",
        "  left_pad, right_pad, top_pad, bottom_pad = 0, 0, 0, 0\n",
        "  if height > width:\n",
        "    left_pad = int((height-width) / 2)\n",
        "    right_pad = height-width-left_pad\n",
        "  elif width > height:\n",
        "    top_pad = int((width-height) / 2)\n",
        "    bottom_pad = width-height-top_pad\n",
        "\n",
        "  img_square = cv.copyMakeBorder(\n",
        "     image,\n",
        "     top_pad,\n",
        "     bottom_pad,\n",
        "     left_pad,\n",
        "     right_pad,\n",
        "     cv.BORDER_CONSTANT,\n",
        "     value=(255,255,255))\n",
        "\n",
        "  video_w = img_square.shape[0]\n",
        "  video_h = img_square.shape[1]\n",
        "  #Finally the processed frames are added to the list\n",
        "  input_images.append(img_square)\n",
        "  \n",
        "video.release()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XvaMlj7Yw6X",
        "colab_type": "text"
      },
      "source": [
        "The MobileNet model file is loaded and unzipped to extract all the files, and output nodes are initialized. The next step will be to input the selected frames into the model for processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gTlqWvCRty2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
        "file_name = 'ssd_mobilenet_v1_coco_2018_01_28.tar.gz'\n",
        "\n",
        "url = base_url + file_name\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "#Extracts the file, checks in computer directory to see what files are in it\n",
        "dir_name = file_name[0:-len('.tar.gz')] #Name of the zip file\n",
        "\n",
        "if os.path.exists(dir_name):\n",
        "  shutil.rmtree(dir_name) \n",
        "\n",
        "tarfile.open(file_name, 'r:gz').extractall('./')\n",
        "\n",
        "#Getting nodes\n",
        "frozen_graph = os.path.join(dir_name, 'frozen_inference_graph.pb')\n",
        "\n",
        "with tf.gfile.FastGFile(frozen_graph,'rb') as f:\n",
        "  graph_def = tf.GraphDef()\n",
        "  graph_def.ParseFromString(f.read())\n",
        "\n",
        "outputs = (\n",
        "  'num_detections',\n",
        "  'detection_classes',\n",
        "  'detection_scores',\n",
        "  'detection_boxes',\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncFS3NDPaHDv",
        "colab_type": "text"
      },
      "source": [
        "Using a TensorFlow session, the list of frames is inputted into the model and the outputs are produced into a list called 'detections.' The items in the list is separated and labeled into Number of Detections, Detection Classes, Detection Scores, and Detection Boxes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoVm45vmSb9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The model is ran through a TensorFlow session\n",
        "with tf.Session() as sess:\n",
        "  sess.graph.as_default()\n",
        "  tf.import_graph_def(graph_def, name='')\n",
        "\n",
        "  detections = sess.run(\n",
        "      [sess.graph.get_tensor_by_name(f'{op}:0') for op in outputs],\n",
        "      feed_dict={ 'image_tensor:0': input_images }\n",
        "  )\n",
        "\n",
        "#Each output node is assigned to more elaborate names\n",
        "num_detections = detections[0]\n",
        "detection_classes = detections[1]\n",
        "detection_scores = detections[2]\n",
        "detection_boxes = detections[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5IH_dMPbesy",
        "colab_type": "text"
      },
      "source": [
        "Instead of hard coding the labels, the .txt file with the ID codes was cleaned up to create a dictionary (referenced above). As the algorithm iterates, it references the dictionary to label corresponding to the ID code (i.e. '3' is a car, and '10' is a traffic light)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi2U5fRHDE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Open the .txt file that contains all the labels\n",
        "f = open(\"labels.txt\", \"r\")\n",
        "\n",
        "#Initialize a dictionary\n",
        "labels = {}\n",
        "\n",
        "#Process the .txt file and establish the classes as the key and the object name as the value\n",
        "for x in f:\n",
        "  key,_,value= x.partition(':')\n",
        "  value,_,_ = value.partition('\\n') \n",
        "  labels[key] = value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVGNVKTTb-0k",
        "colab_type": "text"
      },
      "source": [
        "A new video is created and formatted to write all the frames with the addition of boundary boxes and labels in. The algorithm iterates through each frame, then references the number of objects detected and begins drawing and labeling boundary boxes in each object within the frame. For our sample video, we put white boxes around the cars, and blue frames around non-car objects. \n",
        "\n",
        "The width and height were used to normalized the metrics provided by the Detection Box output. \n",
        "\n",
        "Finally, the video is released for good practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjrDfKl9qTqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Put the name of the new file here:\n",
        "video_name = 'cars-detect.mp4'\n",
        "\n",
        "#Initialize a new video object for the output\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "output_video = cv.VideoWriter(video_name, fourcc, fps, (video_w, video_h))\n",
        "\n",
        "#First for-loop iterates through each frame \n",
        "for x in range(0,len(num_detections)):\n",
        "  frame_copy = np.copy(input_images[x])\n",
        "  height,width,_ = frame_copy.shape\n",
        "  #Second for-loop itertes through each object detected in each frame\n",
        "  for i in range(0,int(num_detections[x])):\n",
        "    #Set a threshold for confidence levels over 30%\n",
        "    if detections[2][0][i] > .3:\n",
        "      #Car objects have their own colored boundary boxes\n",
        "      if detection_classes[x][i] == 3: \n",
        "         color = [250,230,230]\n",
        "      #Other objects have blue colored boxes\n",
        "      else: #etc\n",
        "         color = [255,0,0]\n",
        "      #Dimensions for the boundary boxes are initialized and normalized\n",
        "      left = int(width*detection_boxes[x][i][1])\n",
        "      top = int(height*detection_boxes[x][i][0])\n",
        "      right = int(width*detection_boxes[x][i][3])\n",
        "      bottom = int(height*detection_boxes[x][i][2])\n",
        "      #Boundary boxes are drawn\n",
        "      cv.rectangle(frame_copy,\n",
        "                    (left, top),\n",
        "                    (right, bottom),\n",
        "                    (color),\n",
        "                    2)\n",
        "      #Labels are retrieved from the dictionary\n",
        "      label = labels[str(int(detection_classes[x][i]))]\n",
        "      #Labels are properly assigned to each object\n",
        "      cv.putText(frame_copy, label, (left, top-10), cv.FONT_HERSHEY_TRIPLEX, .5, [255,0,255], 1)\n",
        "\n",
        "  frame_copy = cv.cvtColor(frame_copy, cv.COLOR_BGR2RGB)\n",
        "  \n",
        "  #Processed frames are written into the output video\n",
        "  output_video.write(frame_copy)\n",
        "  plt.subplots()\n",
        "  plt.imshow(frame_copy)\n",
        "  plt.show()\n",
        "\n",
        "#Output video is released as good practice and to free up memory\n",
        "output_video.release()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfjQXXvCcwUx",
        "colab_type": "text"
      },
      "source": [
        "Our final product is a shortened video with relatively accurate labels for each detection. The model works best with simple backgrounds with little to no noise, but even then may make incorrect detections like the side of the highway being a chair."
      ]
    }
  ]
}